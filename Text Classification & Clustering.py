# -*- coding: utf-8 -*-
"""Copy of 21K-3381 Assignment 3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gK5u5wN-pBa11a42a8nvD3bFQuECXZbV

Part 1
"""

import os
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import classification_report

# Function to read documents from local storage
def read_documents_from_folder(folder_path):
    documents = {}
    for filename in os.listdir(folder_path):
        with open(os.path.join(folder_path, filename), "r", encoding="latin-1") as file:
            document_text = file.read()
            documents[int(filename.split('.')[0])] = document_text
    return documents

# Sample data folder path
data_folder_path = "/content/drive/MyDrive/ResearchPapers"

# Read documents from the folder
documents = read_documents_from_folder(data_folder_path)

# Class labels
class_labels = {
    1: "Explainable Artificial Intelligence",
    2: "Explainable Artificial Intelligence",
    3: "Explainable Artificial Intelligence",
    7: "Explainable Artificial Intelligence",
    8: "Heart Failure",
    9: "Heart Failure",
    11: "Heart Failure",
    12: "Time Series Forecasting",
    13: "Time Series Forecasting",
    14: "Time Series Forecasting",
    15: "Time Series Forecasting",
    16: "Time Series Forecasting",
    17: "Transformer Model",
    18: "Transformer Model",
    21: "Transformer Model",
    22: "Feature Selection",
    23: "Feature Selection",
    24: "Feature Selection",
    25: "Feature Selection",
    26: "Feature Selection"
}

# Tokenization and TF calculation
term_frequency = {}
for doc_id, document in documents.items():
    terms = document.split()  # Simple whitespace tokenizer
    for term in terms:
        term_frequency[(doc_id, term)] = term_frequency.get((doc_id, term), 0) + 1

# Calculate IDF
document_count = len(documents)
inverse_document_frequency = {}
for doc_id, document in documents.items():
    terms = set(document.split())
    for term in terms:
        inverse_document_frequency[term] = inverse_document_frequency.get(term, 0) + 1

# Compute TF-IDF
tfidf = {}
for (doc_id, term), freq in term_frequency.items():
    tf = freq / len(documents[doc_id].split())
    idf = np.log(document_count / (1 + inverse_document_frequency.get(term, 0)))
    tfidf[(doc_id, term)] = tf * idf

# Build Vector Space Model
vectors = np.zeros((len(documents), len(inverse_document_frequency)))
doc_ids = sorted(documents.keys())
terms = sorted(inverse_document_frequency.keys())

for i, doc_id in enumerate(doc_ids):
    for j, term in enumerate(terms):
        vectors[i, j] = tfidf.get((doc_id, term), 0)

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(vectors, np.array([class_labels[key] for key in doc_ids]), test_size=0.5, random_state=42)

# Initialize k-NN classifier
k = 3
knn_classifier = KNeighborsClassifier(n_neighbors=k)

# Train the classifier
knn_classifier.fit(X_train, y_train)

# Predict on the test data
y_pred = knn_classifier.predict(X_test)

# Evaluate the classifier
report = classification_report(y_test, y_pred, output_dict=True)

# Print evaluation metrics
print("Accuracy:", report['accuracy'])
for label, metrics in report.items():
    if label != 'accuracy':
        print("Class:", label)
        print(metrics)

"""Part 2"""

import os
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score, adjusted_rand_score
from sklearn.metrics.cluster import contingency_matrix

# Function to read documents from local storage
def read_documents_from_folder(folder_path):
    documents = []
    filenames = []
    class_labels = []
    for filename in os.listdir(folder_path):
        with open(os.path.join(folder_path, filename), 'r', encoding='latin-1') as file:
            document_text = file.read()
            documents.append(document_text)
            filenames.append(filename)
            if int(filename.split('.')[0]) in [1, 2, 3, 7]:# determines the class label based on the filename (extracting the prefix before the '.' character)
                class_labels.append("Explainable Artificial Intelligence")
            elif int(filename.split('.')[0]) in [8, 9, 11]:
                class_labels.append("Heart Failure")
            elif int(filename.split('.')[0]) in [12, 13, 14, 15, 16]:
                class_labels.append("Time Series Forecasting")
            elif int(filename.split('.')[0]) in [17, 18, 21]:
                class_labels.append("Transformer Model")
            elif int(filename.split('.')[0]) in [22, 23, 24, 25, 26]:
                class_labels.append("Feature Selection")
    # print(class_labels)
    return documents, filenames, class_labels

# Sample data folder path
data_folder_path = "/content/drive/MyDrive/ResearchPapers"

# Read documents from the folder
documents, filenames, class_labels = read_documents_from_folder(data_folder_path)

tfidf_vectorizer = TfidfVectorizer()
X = tfidf_vectorizer.fit_transform(documents)# converts the text documents into TF-IDF vectors (X stores all doc vectors)

k = int(input("Enter value of k: "))

kmeans = KMeans(n_clusters=k, n_init=10, random_state=42) #n_init is no of times the algorithm runs for

kmeans.fit(X) #  fits the k means algorithm to the TF-IDF vectors (X) to cluster the documents into k clusters.

# Get cluster labels after training of data
cluster_labels = kmeans.labels_

# Evaluate clustering using purity
def purity_score(y_true, y_pred):
    contingency = contingency_matrix(y_true, y_pred)
    return np.sum(np.amax(contingency, axis=0)) / np.sum(contingency)

# Evaluate clustering using silhouette score
silhouette = silhouette_score(X, cluster_labels)

# computes a similarity measure,RI between two clusterings by considering all pairs of samples and counting pairs that are assigned in the same or different clusters in the predicted and true clusterings.
random_index = adjusted_rand_score(class_labels, cluster_labels)
#0 is random labelling and 1 is when clusterings are identical

# Print evaluation metrics
print("Purity:", purity_score(y_true=class_labels, y_pred=cluster_labels))
print("Silhouette Score:", silhouette)
print("Random Index:", random_index)